"""
Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù†Ù…ÙˆØ°Ø¬ GSE
Advanced Optimization Engine for GSE Model
"""

import numpy as np
from scipy.optimize import minimize, differential_evolution, basinhopping
from scipy.optimize import dual_annealing
import warnings
warnings.filterwarnings('ignore')

class GSEOptimizer:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù†Ù…ÙˆØ°Ø¬ GSE"""
    
    def __init__(self, model):
        self.model = model
        self.optimization_history = []
        self.best_loss = float('inf')
        self.best_params = None
        
    def loss_function(self, param_vector, x_data, y_true, regularization=0.001):
        """
        Ø¯Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ù…Ø¹ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…
        """
        try:
            # ØªØ­Ø¯ÙŠØ« Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            self.model.set_params_from_vector(param_vector)
            
            # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            y_pred = self.model.evaluate(x_data)
            
            # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø®Ø·Ø£ Ø§Ù„ØªØ±Ø¨ÙŠØ¹ÙŠ
            mse = np.mean((y_true - y_pred) ** 2)
            
            # Ø¥Ø¶Ø§ÙØ© ØªÙ†Ø¸ÙŠÙ… L2 Ù„Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
            l2_penalty = regularization * np.sum(param_vector ** 2)
            
            # Ø¥Ø¶Ø§ÙØ© ØªÙ†Ø¸ÙŠÙ… Ù„Ù„ØªØ¹Ù‚ÙŠØ¯ (Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª)
            complexity_penalty = 0.01 * len(self.model.sigmoid_components)
            
            total_loss = mse + l2_penalty + complexity_penalty
            
            # Ø­ÙØ¸ Ø§Ù„ØªØ§Ø±ÙŠØ®
            self.optimization_history.append(total_loss)
            
            # ØªØ­Ø¯ÙŠØ« Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø©
            if total_loss < self.best_loss:
                self.best_loss = total_loss
                self.best_params = param_vector.copy()
            
            return total_loss
            
        except Exception as e:
            # ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£ØŒ Ø¥Ø±Ø¬Ø§Ø¹ Ù‚ÙŠÙ…Ø© ÙƒØ¨ÙŠØ±Ø©
            return 1e10
    
    def get_parameter_bounds(self):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù„Ù„ØªØ­Ø³ÙŠÙ†"""
        bounds = []
        
        for component in self.model.sigmoid_components:
            # Ø­Ø¯ÙˆØ¯ Ù„ÙƒÙ„ Ù…ÙƒÙˆÙ† Ø³ÙŠØ¬Ù…ÙˆÙŠØ¯
            bounds.extend([
                (-10, 10),    # alpha_real
                (-10, 10),    # alpha_imag  
                (0.1, 5.0),   # n (Ø§Ù„Ø£Ø³)
                (-5, 5),      # z_real
                (-5, 5),      # z_imag
                (-50, 50)     # x0 (Ø§Ù„Ø¥Ø²Ø§Ø­Ø©)
            ])
        
        # Ø­Ø¯ÙˆØ¯ Ù„Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø®Ø·ÙŠØ©
        bounds.extend([(-5, 5), (-5, 5)])  # beta, gamma
        
        return bounds
    
    def generate_initial_population(self, pop_size=50):
        """ØªÙˆÙ„ÙŠØ¯ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø£ÙˆÙ„ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª"""
        bounds = self.get_parameter_bounds()
        population = []
        
        for _ in range(pop_size):
            individual = []
            for lower, upper in bounds:
                individual.append(np.random.uniform(lower, upper))
            population.append(np.array(individual))
        
        return population
    
    def optimize_differential_evolution(self, x_data, y_true, max_iter=1000, 
                                      popsize=15, seed=42, verbose=True):
        """ØªØ­Ø³ÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„ØªÙØ§Ø¶Ù„ÙŠ"""
        if verbose:
            print("ğŸ§¬ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø¨Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„ØªÙØ§Ø¶Ù„ÙŠ...")
        
        bounds = self.get_parameter_bounds()
        
        def objective(params):
            return self.loss_function(params, x_data, y_true)
        
        result = differential_evolution(
            objective,
            bounds,
            maxiter=max_iter,
            popsize=popsize,
            seed=seed,
            disp=verbose,
            atol=1e-8,
            tol=1e-8,
            workers=1  # ØªØ¬Ù†Ø¨ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ØªÙˆØ§Ø²ÙŠ
        )
        
        if verbose:
            print(f"âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„ØªÙØ§Ø¶Ù„ÙŠ. Ø£ÙØ¶Ù„ Ø®Ø·Ø£: {result.fun:.8f}")
        
        return result
    
    def optimize_simulated_annealing(self, x_data, y_true, max_iter=1000, verbose=True):
        """ØªØ­Ø³ÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¨Ø±ÙŠØ¯ Ø§Ù„Ù…Ø­Ø§ÙƒÙŠ"""
        if verbose:
            print("ğŸŒ¡ï¸ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø¨Ø§Ù„ØªØ¨Ø±ÙŠØ¯ Ø§Ù„Ù…Ø­Ø§ÙƒÙŠ...")
        
        bounds = self.get_parameter_bounds()
        
        def objective(params):
            return self.loss_function(params, x_data, y_true)
        
        result = dual_annealing(
            objective,
            bounds,
            maxiter=max_iter,
            seed=42
        )
        
        if verbose:
            print(f"âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ¨Ø±ÙŠØ¯ Ø§Ù„Ù…Ø­Ø§ÙƒÙŠ. Ø£ÙØ¶Ù„ Ø®Ø·Ø£: {result.fun:.8f}")
        
        return result
    
    def optimize_basin_hopping(self, x_data, y_true, n_iter=100, verbose=True):
        """ØªØ­Ø³ÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Basin Hopping"""
        if verbose:
            print("ğŸ”ï¸ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø¨Ù€ Basin Hopping...")
        
        # Ù†Ù‚Ø·Ø© Ø¨Ø¯Ø§ÙŠØ© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
        initial_params = self.model.get_params_vector()
        bounds = self.get_parameter_bounds()
        
        def objective(params):
            return self.loss_function(params, x_data, y_true)
        
        # Ø®ÙŠØ§Ø±Ø§Øª Ù„Ù„Ù…Ø­Ø³Ù† Ø§Ù„Ù…Ø­Ù„ÙŠ
        minimizer_kwargs = {
            "method": "L-BFGS-B",
            "bounds": bounds,
            "options": {"maxiter": 100}
        }
        
        result = basinhopping(
            objective,
            initial_params,
            niter=n_iter,
            minimizer_kwargs=minimizer_kwargs,
            seed=42,
            disp=verbose
        )
        
        if verbose:
            print(f"âœ… Ø§Ù†ØªÙ‡Ù‰ Basin Hopping. Ø£ÙØ¶Ù„ Ø®Ø·Ø£: {result.fun:.8f}")
        
        return result
    
    def optimize_hybrid(self, x_data, y_true, verbose=True):
        """ØªØ­Ø³ÙŠÙ† Ù‡Ø¬ÙŠÙ† ÙŠØ¬Ù…Ø¹ Ø¹Ø¯Ø© Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª"""
        if verbose:
            print("ğŸ”„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù‡Ø¬ÙŠÙ†...")
        
        results = []
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„ØªÙØ§Ø¶Ù„ÙŠ (Ø§Ø³ØªÙƒØ´Ø§Ù Ø¹Ø§Ù…)
        if verbose:
            print("Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„ØªÙØ§Ø¶Ù„ÙŠ...")
        result1 = self.optimize_differential_evolution(
            x_data, y_true, max_iter=200, verbose=False
        )
        results.append(('differential_evolution', result1))
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø§Ù„ØªØ¨Ø±ÙŠØ¯ Ø§Ù„Ù…Ø­Ø§ÙƒÙŠ (ØªØ­Ø³ÙŠÙ† Ù…ØªÙˆØ³Ø·)
        if verbose:
            print("Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø§Ù„ØªØ¨Ø±ÙŠØ¯ Ø§Ù„Ù…Ø­Ø§ÙƒÙŠ...")
        result2 = self.optimize_simulated_annealing(
            x_data, y_true, max_iter=200, verbose=False
        )
        results.append(('simulated_annealing', result2))
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Basin Hopping (ØªØ­Ø³ÙŠÙ† Ù…Ø­Ù„ÙŠ)
        if verbose:
            print("Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Basin Hopping...")
        result3 = self.optimize_basin_hopping(
            x_data, y_true, n_iter=50, verbose=False
        )
        results.append(('basin_hopping', result3))
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø©
        best_result = min(results, key=lambda x: x[1].fun)
        
        if verbose:
            print(f"âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù‡Ø¬ÙŠÙ†.")
            print(f"Ø£ÙØ¶Ù„ Ø·Ø±ÙŠÙ‚Ø©: {best_result[0]}")
            print(f"Ø£ÙØ¶Ù„ Ø®Ø·Ø£: {best_result[1].fun:.8f}")
        
        return best_result[1]
    
    def train_model(self, x_data, y_true, method='hybrid', **kwargs):
        """
        ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
        """
        print(f"ğŸš€ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ GSE...")
        print(f"ğŸ“Š Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {len(x_data)} Ù†Ù‚Ø·Ø©")
        print(f"ğŸ¯ Ø§Ù„Ù‡Ø¯Ù: {method}")
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„ØªØ§Ø±ÙŠØ®
        self.optimization_history = []
        self.best_loss = float('inf')
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ­Ø³ÙŠÙ†
        if method == 'differential_evolution':
            result = self.optimize_differential_evolution(x_data, y_true, **kwargs)
        elif method == 'simulated_annealing':
            result = self.optimize_simulated_annealing(x_data, y_true, **kwargs)
        elif method == 'basin_hopping':
            result = self.optimize_basin_hopping(x_data, y_true, **kwargs)
        elif method == 'hybrid':
            result = self.optimize_hybrid(x_data, y_true, **kwargs)
        else:
            raise ValueError(f"Ø·Ø±ÙŠÙ‚Ø© ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø©: {method}")
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø£ÙØ¶Ù„ Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        self.model.set_params_from_vector(result.x)
        self.model.trained = True
        self.model.training_history = self.optimization_history
        
        print(f"âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­!")
        print(f"ğŸ“ˆ Ø§Ù„Ø®Ø·Ø£ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: {result.fun:.8f}")
        print(f"ğŸ”„ Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: {len(self.optimization_history)}")
        
        return result
    
    def evaluate_convergence(self):
        """ØªÙ‚ÙŠÙŠÙ… ØªÙ‚Ø§Ø±Ø¨ Ø§Ù„ØªØ­Ø³ÙŠÙ†"""
        if len(self.optimization_history) < 10:
            return "ØºÙŠØ± ÙƒØ§ÙÙŠ Ù„Ù„ØªÙ‚ÙŠÙŠÙ…"
        
        # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ­Ø³Ù† ÙÙŠ Ø¢Ø®Ø± 100 ØªÙƒØ±Ø§Ø±
        recent_history = self.optimization_history[-100:]
        improvement_rate = (recent_history[0] - recent_history[-1]) / recent_history[0]
        
        if improvement_rate < 0.001:
            return "Ù…ØªÙ‚Ø§Ø±Ø¨"
        elif improvement_rate < 0.01:
            return "ØªÙ‚Ø§Ø±Ø¨ Ø¨Ø·ÙŠØ¡"
        else:
            return "ÙŠØªØ­Ø³Ù†"
    
    def get_optimization_summary(self):
        """Ù…Ù„Ø®Øµ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ­Ø³ÙŠÙ†"""
        return {
            'total_iterations': len(self.optimization_history),
            'best_loss': self.best_loss,
            'final_loss': self.optimization_history[-1] if self.optimization_history else None,
            'convergence_status': self.evaluate_convergence(),
            'improvement_ratio': (self.optimization_history[0] - self.best_loss) / self.optimization_history[0] if self.optimization_history else 0
        }
