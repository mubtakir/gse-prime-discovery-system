#!/usr/bin/env python3
"""
Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø¨ÙŠØ±/Ø§Ù„Ù…Ø³ØªÙƒØ´Ù Ù„Ù†Ù…ÙˆØ°Ø¬ GSE
Ù…Ø³ØªÙˆØ­Ù‰ Ù…Ù† Ù†Ø¸Ø§Ù… Baserah Ù…Ø¹ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø«

Ø§Ù„Ù…ÙŠØ²Ø§Øª:
- ÙˆØ¶Ø¹ Ø§Ù„Ø®Ø¨ÙŠØ±: ØªØ­Ù„ÙŠÙ„ Ø°ÙƒÙŠ Ù„Ù„Ø£Ù†Ù…Ø§Ø· ÙˆØ§Ù‚ØªØ±Ø§Ø­ Ø£ÙØ¶Ù„ Ù…Ø¹Ø§Ù…Ù„Ø§Øª
- ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø³ØªÙƒØ´Ù: Ø§Ø³ØªÙƒØ´Ø§Ù Ø£Ù†Ù…Ø§Ø· Ø¬Ø¯ÙŠØ¯Ø© ÙˆØªØ¬Ø±ÙŠØ¨ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…Ø¨ØªÙƒØ±Ø©
- ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø« ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù
- ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„ØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
"""

import numpy as np
import random
from typing import Dict, List, Tuple, Optional, Any, Set
from dataclasses import dataclass
from enum import Enum
import logging
from datetime import datetime
import json

try:
    from .three_theories_core import ThreeTheoriesIntegrator
    from .adaptive_equations import AdaptiveGSEEquation, AdaptationDirection
except ImportError:
    from three_theories_core import ThreeTheoriesIntegrator
    from adaptive_equations import AdaptiveGSEEquation, AdaptationDirection

logger = logging.getLogger(__name__)

class ExpertMode(Enum):
    """Ø£ÙˆØ¶Ø§Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø¨ÙŠØ±"""
    ANALYSIS = "analysis"          # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø·
    OPTIMIZATION = "optimization"  # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
    PREDICTION = "prediction"      # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø£Ø¯Ø§Ø¡
    DIAGNOSIS = "diagnosis"        # ØªØ´Ø®ÙŠØµ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„

class ExplorerMode(Enum):
    """Ø£ÙˆØ¶Ø§Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø³ØªÙƒØ´Ù"""
    RANDOM = "random"              # Ø§Ø³ØªÙƒØ´Ø§Ù Ø¹Ø´ÙˆØ§Ø¦ÙŠ
    GUIDED = "guided"              # Ø§Ø³ØªÙƒØ´Ø§Ù Ù…ÙˆØ¬Ù‡
    FOCUSED = "focused"            # Ø§Ø³ØªÙƒØ´Ø§Ù Ù…Ø±ÙƒØ²
    CREATIVE = "creative"          # Ø§Ø³ØªÙƒØ´Ø§Ù Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ

@dataclass
class ExplorationConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù"""
    exploration_radius: float = 1.0
    max_explorations: int = 50
    creativity_factor: float = 0.2
    risk_tolerance: float = 0.1
    learning_rate: float = 0.01

@dataclass
class ExpertAnalysis:
    """Ù†ØªÙŠØ¬Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø¨ÙŠØ±"""
    pattern_type: str
    confidence: float
    recommendations: List[str]
    optimal_parameters: Dict[str, float]
    risk_assessment: str
    expected_improvement: float

@dataclass
class ExplorationResult:
    """Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù"""
    discovered_patterns: List[Dict]
    new_parameters: List[Dict]
    performance_scores: List[float]
    exploration_path: List[Dict]
    success_rate: float

class GSEExpertSystem:
    """
    Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø¨ÙŠØ± Ù„Ù€ GSE
    
    ÙŠØ­Ù„Ù„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· ÙˆÙŠÙ‚ØªØ±Ø­ Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø¨Ø±Ø© Ø§Ù„Ù…ØªØ±Ø§ÙƒÙ…Ø©
    """
    
    def __init__(self):
        self.knowledge_base = {}
        self.pattern_library = {}
        self.experience_history = []
        self.theories_integrator = ThreeTheoriesIntegrator()
        
        # Ù‚Ø§Ø¹Ø¯Ø© Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
        self._initialize_pattern_knowledge()
        
        logger.info("ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø¨ÙŠØ± GSE")
    
    def _initialize_pattern_knowledge(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø£Ù†Ù…Ø§Ø·"""
        
        self.pattern_library = {
            'linear': {
                'description': 'Ù†Ù…Ø· Ø®Ø·ÙŠ',
                'optimal_components': [{'type': 'linear', 'beta': 1.0, 'gamma': 0.0}],
                'confidence_threshold': 0.9
            },
            'exponential': {
                'description': 'Ù†Ù…Ø· Ø£Ø³ÙŠ',
                'optimal_components': [{'type': 'sigmoid', 'alpha': 1.0, 'k': 2.0, 'x0': 0.0}],
                'confidence_threshold': 0.8
            },
            'oscillatory': {
                'description': 'Ù†Ù…Ø· Ù…ØªØ°Ø¨Ø°Ø¨',
                'optimal_components': [
                    {'type': 'sigmoid', 'alpha': 1.0, 'k': 1.0, 'x0': -1.0},
                    {'type': 'sigmoid', 'alpha': -1.0, 'k': 1.0, 'x0': 1.0}
                ],
                'confidence_threshold': 0.7
            },
            'prime_like': {
                'description': 'Ù†Ù…Ø· Ø´Ø¨ÙŠÙ‡ Ø¨Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø£ÙˆÙ„ÙŠØ©',
                'optimal_components': [
                    {'type': 'sigmoid', 'alpha': 1.0, 'k': 0.5, 'x0': 2.0},
                    {'type': 'sigmoid', 'alpha': 0.8, 'k': 0.3, 'x0': 3.0},
                    {'type': 'linear', 'beta': 0.1, 'gamma': 0.0}
                ],
                'confidence_threshold': 0.85
            }
        }
    
    def analyze_data_pattern(self, x_data: np.ndarray, y_data: np.ndarray) -> ExpertAnalysis:
        """ØªØ­Ù„ÙŠÙ„ Ù†Ù…Ø· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªÙ‚Ø¯ÙŠÙ… ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø®Ø¨ÙŠØ±"""
        
        logger.info("Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ù†Ù…Ø· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        
        # ØªØ­Ù„ÙŠÙ„ Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        data_characteristics = self._analyze_data_characteristics(x_data, y_data)
        
        # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·
        pattern_type = self._identify_pattern_type(data_characteristics)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        confidence = self._calculate_analysis_confidence(data_characteristics, pattern_type)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø« ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        theory_insights = self._apply_theories_to_analysis(x_data, y_data)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª
        recommendations = self._generate_recommendations(pattern_type, theory_insights)
        
        # Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø«Ù„Ù‰
        optimal_parameters = self._suggest_optimal_parameters(pattern_type, data_characteristics)
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø§Ø·Ø±
        risk_assessment = self._assess_risks(pattern_type, data_characteristics)
        
        # ØªÙ‚Ø¯ÙŠØ± Ø§Ù„ØªØ­Ø³Ù† Ø§Ù„Ù…ØªÙˆÙ‚Ø¹
        expected_improvement = self._estimate_improvement(pattern_type, confidence)
        
        analysis = ExpertAnalysis(
            pattern_type=pattern_type,
            confidence=confidence,
            recommendations=recommendations,
            optimal_parameters=optimal_parameters,
            risk_assessment=risk_assessment,
            expected_improvement=expected_improvement
        )
        
        # Ø­ÙØ¸ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
        self._update_knowledge_base(analysis, x_data, y_data)
        
        logger.info(f"Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„: Ù†Ù…Ø·={pattern_type}, Ø«Ù‚Ø©={confidence:.2%}")
        
        return analysis
    
    def _analyze_data_characteristics(self, x_data: np.ndarray, y_data: np.ndarray) -> Dict[str, float]:
        """ØªØ­Ù„ÙŠÙ„ Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        
        characteristics = {}
        
        # Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        characteristics['mean'] = np.mean(y_data)
        characteristics['std'] = np.std(y_data)
        characteristics['min'] = np.min(y_data)
        characteristics['max'] = np.max(y_data)
        characteristics['range'] = characteristics['max'] - characteristics['min']
        
        # Ø®ØµØ§Ø¦Øµ Ø§Ù„ØªØºÙŠØ±
        if len(y_data) > 1:
            differences = np.diff(y_data)
            characteristics['monotonicity'] = np.sum(np.sign(differences)) / len(differences)
            characteristics['volatility'] = np.std(differences)
            characteristics['trend'] = np.polyfit(x_data, y_data, 1)[0]
        
        # Ø®ØµØ§Ø¦Øµ Ø§Ù„ØªØ°Ø¨Ø°Ø¨
        if len(y_data) > 2:
            second_diff = np.diff(y_data, 2)
            characteristics['curvature'] = np.mean(np.abs(second_diff))
            characteristics['oscillation'] = np.std(second_diff)
        
        # Ø®ØµØ§Ø¦Øµ Ø§Ù„ØªÙˆØ²ÙŠØ¹
        characteristics['skewness'] = self._calculate_skewness(y_data)
        characteristics['kurtosis'] = self._calculate_kurtosis(y_data)
        
        return characteristics
    
    def _identify_pattern_type(self, characteristics: Dict[str, float]) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø®ØµØ§Ø¦Øµ"""
        
        # Ù‚ÙˆØ§Ø¹Ø¯ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù…Ø·
        if abs(characteristics.get('monotonicity', 0)) > 0.8:
            if abs(characteristics.get('trend', 0)) > 0.1:
                return 'linear'
            else:
                return 'exponential'
        
        elif characteristics.get('oscillation', 0) > characteristics.get('std', 1) * 0.5:
            return 'oscillatory'
        
        elif characteristics.get('curvature', 0) > 0.1:
            # ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø´Ø¨ÙŠÙ‡ Ø¨Ù†Ù…Ø· Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
            if self._is_prime_like_pattern(characteristics):
                return 'prime_like'
            else:
                return 'exponential'
        
        else:
            return 'linear'
    
    def _is_prime_like_pattern(self, characteristics: Dict[str, float]) -> bool:
        """ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Ù…Ø· Ø´Ø¨ÙŠÙ‡ Ø¨Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø£ÙˆÙ„ÙŠØ©"""
        
        # Ø®ØµØ§Ø¦Øµ Ù…Ù…ÙŠØ²Ø© Ù„Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
        irregular_spacing = characteristics.get('volatility', 0) > 0.5
        moderate_growth = 0.1 < characteristics.get('trend', 0) < 2.0
        non_uniform_distribution = characteristics.get('skewness', 0) > 0.2
        
        return irregular_spacing and moderate_growth and non_uniform_distribution

    def _calculate_analysis_confidence(self, characteristics: Dict[str, float], pattern_type: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""

        confidence_factors = []

        # Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙˆØ¶ÙˆØ­ Ø§Ù„Ù†Ù…Ø·
        if pattern_type in self.pattern_library:
            threshold = self.pattern_library[pattern_type]['confidence_threshold']
            confidence_factors.append(threshold)

        # Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        data_quality = 1.0 - min(characteristics.get('volatility', 0) / 2.0, 0.5)
        confidence_factors.append(data_quality)

        # Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§ØªØ³Ø§Ù‚ Ø§Ù„Ù†Ù…Ø·
        consistency = 1.0 - abs(characteristics.get('skewness', 0)) / 3.0
        confidence_factors.append(max(0.1, consistency))

        return np.mean(confidence_factors)

    def _generate_recommendations(self, pattern_type: str, theory_insights: Dict[str, Any]) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""

        recommendations = []

        if pattern_type == 'linear':
            recommendations.append("Ø§Ø³ØªØ®Ø¯Ù… Ù…ÙƒÙˆÙ† Ø®Ø·ÙŠ Ø¨Ø³ÙŠØ·")
            recommendations.append("Ù‚Ù„Ù„ Ø¹Ø¯Ø¯ Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø³ÙŠØ¬Ù…ÙˆÙŠØ¯")
        elif pattern_type == 'exponential':
            recommendations.append("Ø§Ø³ØªØ®Ø¯Ù… Ù…ÙƒÙˆÙ† Ø³ÙŠØ¬Ù…ÙˆÙŠØ¯ ÙˆØ§Ø­Ø¯ Ù‚ÙˆÙŠ")
            recommendations.append("Ø§Ø¶Ø¨Ø· Ù…Ø¹Ø§Ù…Ù„ k Ù„ÙŠÙƒÙˆÙ† Ø£ÙƒØ¨Ø± Ù…Ù† 1")
        elif pattern_type == 'prime_like':
            recommendations.append("Ø§Ø³ØªØ®Ø¯Ù… Ø¹Ø¯Ø© Ù…ÙƒÙˆÙ†Ø§Øª Ø³ÙŠØ¬Ù…ÙˆÙŠØ¯")
            recommendations.append("Ø·Ø¨Ù‚ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø« Ù„Ù„ØªØ­Ø³ÙŠÙ†")
            recommendations.append("Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙƒÙŠÙ Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ")

        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª
        if theory_insights.get('balance_score', 0.5) < 0.3:
            recommendations.append("Ø·Ø¨Ù‚ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ØªÙˆØ§Ø²Ù† Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±")

        if theory_insights.get('perpendicular_strength', 0) > 1.0:
            recommendations.append("Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ØªØ¹Ø§Ù…Ø¯ Ù„Ù„Ø§Ø³ØªÙƒØ´Ø§Ù")

        return recommendations

    def _suggest_optimal_parameters(self, pattern_type: str, characteristics: Dict[str, float]) -> Dict[str, float]:
        """Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø«Ù„Ù‰"""

        if pattern_type in self.pattern_library:
            base_components = self.pattern_library[pattern_type]['optimal_components']

            # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            suggested_params = {}

            for i, component in enumerate(base_components):
                if component['type'] == 'sigmoid':
                    # ØªØ¹Ø¯ÙŠÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†Ø·Ø§Ù‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                    range_factor = characteristics.get('range', 1.0)
                    suggested_params[f'alpha_{i}'] = component['alpha'] * min(range_factor, 3.0)
                    suggested_params[f'k_{i}'] = component['k']
                    suggested_params[f'x0_{i}'] = component['x0']
                elif component['type'] == 'linear':
                    trend = characteristics.get('trend', 0.0)
                    suggested_params[f'beta_{i}'] = component['beta'] * (1 + trend)
                    suggested_params[f'gamma_{i}'] = component['gamma']

            return suggested_params

        return {}

    def _assess_risks(self, pattern_type: str, characteristics: Dict[str, float]) -> str:
        """ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø§Ø·Ø±"""

        risk_factors = []

        # Ù…Ø®Ø§Ø·Ø± Ø¹Ø¯Ù… Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±
        if characteristics.get('volatility', 0) > 1.0:
            risk_factors.append("Ø¹Ø¯Ù… Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø¹Ø§Ù„ÙŠ")

        # Ù…Ø®Ø§Ø·Ø± Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ø²Ø§Ø¦Ø¯
        if pattern_type == 'oscillatory':
            risk_factors.append("ØªØ¹Ù‚ÙŠØ¯ ÙÙŠ Ø§Ù„Ù†Ù…Ø·")

        # Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø¯ÙˆØ¯Ø©
        if characteristics.get('range', 1.0) < 0.5:
            risk_factors.append("Ù†Ø·Ø§Ù‚ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ø¯ÙˆØ¯")

        if not risk_factors:
            return "Ù…Ø®Ø§Ø·Ø± Ù…Ù†Ø®ÙØ¶Ø©"
        elif len(risk_factors) == 1:
            return f"Ù…Ø®Ø§Ø·Ø± Ù…ØªÙˆØ³Ø·Ø©: {risk_factors[0]}"
        else:
            return f"Ù…Ø®Ø§Ø·Ø± Ø¹Ø§Ù„ÙŠØ©: {', '.join(risk_factors)}"

    def _estimate_improvement(self, pattern_type: str, confidence: float) -> float:
        """ØªÙ‚Ø¯ÙŠØ± Ø§Ù„ØªØ­Ø³Ù† Ø§Ù„Ù…ØªÙˆÙ‚Ø¹"""

        base_improvement = 0.1  # ØªØ­Ø³Ù† Ø£Ø³Ø§Ø³ÙŠ 10%

        # ØªØ­Ø³Ù† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·
        pattern_multipliers = {
            'linear': 1.5,
            'exponential': 1.2,
            'oscillatory': 0.8,
            'prime_like': 1.0
        }

        pattern_factor = pattern_multipliers.get(pattern_type, 1.0)
        confidence_factor = confidence

        estimated_improvement = base_improvement * pattern_factor * confidence_factor

        return min(estimated_improvement, 0.5)  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 50%

    def _update_knowledge_base(self, analysis: 'ExpertAnalysis', x_data: np.ndarray, y_data: np.ndarray):
        """ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©"""

        knowledge_entry = {
            'pattern_type': analysis.pattern_type,
            'confidence': analysis.confidence,
            'data_size': len(x_data),
            'data_range': np.max(x_data) - np.min(x_data),
            'target_range': np.max(y_data) - np.min(y_data),
            'timestamp': datetime.now(),
            'recommendations': analysis.recommendations
        }

        # Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ù‚Ø§Ø¹Ø¯Ø©
        if analysis.pattern_type not in self.knowledge_base:
            self.knowledge_base[analysis.pattern_type] = []

        self.knowledge_base[analysis.pattern_type].append(knowledge_entry)

        # Ø­ÙØ¸ ÙÙŠ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø®Ø¨Ø±Ø©
        self.experience_history.append(knowledge_entry)

    def _apply_theories_to_analysis(self, x_data: np.ndarray, y_data: np.ndarray) -> Dict[str, Any]:
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø« ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        
        insights = {}
        
        # Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ØªÙˆØ§Ø²Ù†: ØªØ­Ù„ÙŠÙ„ ØªÙˆØ§Ø²Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        positive_values = y_data[y_data > 0]
        negative_values = y_data[y_data < 0]
        
        if len(positive_values) > 0 and len(negative_values) > 0:
            balance_score = self.theories_integrator.zero_duality.calculate_balance_point(
                np.sum(positive_values), np.sum(np.abs(negative_values))
            )
            insights['balance_score'] = balance_score
        
        # Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ØªØ¹Ø§Ù…Ø¯: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…ØªØ¹Ø§Ù…Ø¯Ø©
        if len(y_data) > 1:
            gradient = np.gradient(y_data)
            perpendicular_strength = np.std(gradient)
            insights['perpendicular_strength'] = perpendicular_strength
        
        # Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ÙØªØ§Ø¦Ù„: ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ±Ø§Ø¨Ø·
        if len(y_data) > 2:
            correlation_strength = np.corrcoef(x_data, y_data)[0, 1]
            insights['correlation_strength'] = abs(correlation_strength)
        
        return insights
    
    def _calculate_skewness(self, data: np.ndarray) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù"""
        mean = np.mean(data)
        std = np.std(data)
        if std == 0:
            return 0
        return np.mean(((data - mean) / std) ** 3)
    
    def _calculate_kurtosis(self, data: np.ndarray) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙÙ„Ø·Ø­"""
        mean = np.mean(data)
        std = np.std(data)
        if std == 0:
            return 0
        return np.mean(((data - mean) / std) ** 4) - 3

class GSEExplorerSystem:
    """
    Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø³ØªÙƒØ´Ù Ù„Ù€ GSE
    
    ÙŠØ³ØªÙƒØ´Ù Ø£Ù†Ù…Ø§Ø· Ø¬Ø¯ÙŠØ¯Ø© ÙˆÙŠØ¬Ø±Ø¨ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…Ø¨ØªÙƒØ±Ø©
    """
    
    def __init__(self, config: ExplorationConfig = None):
        self.config = config or ExplorationConfig()
        self.exploration_history = []
        self.discovered_patterns = []
        self.theories_integrator = ThreeTheoriesIntegrator()
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù
        self.total_explorations = 0
        self.successful_explorations = 0
        
        logger.info("ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø³ØªÙƒØ´Ù GSE")
    
    def explore_parameter_space(self, base_equation: AdaptiveGSEEquation,
                               x_data: np.ndarray, y_data: np.ndarray,
                               mode: ExplorerMode = ExplorerMode.GUIDED) -> ExplorationResult:
        """Ø§Ø³ØªÙƒØ´Ø§Ù ÙØ¶Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª"""
        
        logger.info(f"Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù: ÙˆØ¶Ø¹={mode.value}")
        
        discovered_patterns = []
        new_parameters = []
        performance_scores = []
        exploration_path = []
        
        base_performance = base_equation.calculate_error(x_data, y_data)
        
        for i in range(self.config.max_explorations):
            # ØªÙˆÙ„ÙŠØ¯ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ø­Ø³Ø¨ Ø§Ù„ÙˆØ¶Ø¹
            if mode == ExplorerMode.RANDOM:
                new_params = self._random_exploration(base_equation)
            elif mode == ExplorerMode.GUIDED:
                new_params = self._guided_exploration(base_equation, x_data, y_data)
            elif mode == ExplorerMode.FOCUSED:
                new_params = self._focused_exploration(base_equation, x_data, y_data)
            else:  # CREATIVE
                new_params = self._creative_exploration(base_equation)
            
            # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
            test_equation = self._create_test_equation(new_params)
            performance = test_equation.calculate_error(x_data, y_data)
            
            # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            exploration_step = {
                'step': i,
                'parameters': new_params,
                'performance': performance,
                'improvement': base_performance - performance,
                'mode': mode.value
            }
            
            exploration_path.append(exploration_step)
            
            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø£ÙØ¶Ù„ØŒ Ø§Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø©
            if performance < base_performance * (1 + self.config.risk_tolerance):
                new_parameters.append(new_params)
                performance_scores.append(performance)
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù…ÙƒØªØ´Ù
                pattern = self._analyze_discovered_pattern(new_params, performance)
                discovered_patterns.append(pattern)
                
                self.successful_explorations += 1
            
            self.total_explorations += 1
        
        # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­
        success_rate = self.successful_explorations / max(1, self.total_explorations)
        
        result = ExplorationResult(
            discovered_patterns=discovered_patterns,
            new_parameters=new_parameters,
            performance_scores=performance_scores,
            exploration_path=exploration_path,
            success_rate=success_rate
        )
        
        # Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù
        self.exploration_history.append(result)
        
        logger.info(f"Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù: Ø§ÙƒØªØ´Ø§ÙØ§Øª={len(discovered_patterns)}, Ù†Ø¬Ø§Ø­={success_rate:.2%}")
        
        return result
    
    def _random_exploration(self, base_equation: AdaptiveGSEEquation) -> Dict[str, Any]:
        """Ø§Ø³ØªÙƒØ´Ø§Ù Ø¹Ø´ÙˆØ§Ø¦ÙŠ"""
        
        new_params = {'components': []}
        
        # Ø¹Ø¯Ø¯ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù…Ù† Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
        num_components = random.randint(1, 4)
        
        for _ in range(num_components):
            component_type = random.choice(['sigmoid', 'linear'])
            
            if component_type == 'sigmoid':
                component = {
                    'type': 'sigmoid',
                    'alpha': random.uniform(0.1, 3.0),
                    'k': random.uniform(0.1, 5.0),
                    'x0': random.uniform(-2.0, 2.0)
                }
            else:
                component = {
                    'type': 'linear',
                    'beta': random.uniform(-2.0, 2.0),
                    'gamma': random.uniform(-1.0, 1.0)
                }
            
            new_params['components'].append(component)
        
        return new_params
    
    def _guided_exploration(self, base_equation: AdaptiveGSEEquation,
                          x_data: np.ndarray, y_data: np.ndarray) -> Dict[str, Any]:
        """Ø§Ø³ØªÙƒØ´Ø§Ù Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø«"""
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø« Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù
        base_components = base_equation.components
        
        if not base_components:
            return self._random_exploration(base_equation)
        
        new_params = {'components': []}
        
        for component in base_components:
            new_component = component.copy()
            
            if component['type'] == 'sigmoid':
                # ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ØªÙˆØ§Ø²Ù†
                balance_factor = self.theories_integrator.zero_duality.calculate_balance_point(
                    component['alpha'], 1.0
                )
                new_component['alpha'] *= balance_factor
                
                # ØªØ·Ø¨ÙŠÙ‚ Ø§Ø³ØªÙƒØ´Ø§Ù Ù…ØªØ¹Ø§Ù…Ø¯
                k_perturbation = random.uniform(-0.5, 0.5)
                new_component['k'] = max(0.1, component['k'] + k_perturbation)
                
            elif component['type'] == 'linear':
                # ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ø³ÙŠÙ† Ø®Ø·ÙŠ Ù…ÙˆØ¬Ù‡
                beta_perturbation = random.uniform(-0.2, 0.2)
                new_component['beta'] += beta_perturbation
            
            new_params['components'].append(new_component)
        
        return new_params

    def _focused_exploration(self, base_equation: AdaptiveGSEEquation,
                           x_data: np.ndarray, y_data: np.ndarray) -> Dict[str, Any]:
        """Ø§Ø³ØªÙƒØ´Ø§Ù Ù…Ø±ÙƒØ² Ø¹Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù…Ù„ ÙˆØ§Ø­Ø¯"""

        if not base_equation.components:
            return self._random_exploration(base_equation)

        # Ø§Ø®ØªÙŠØ§Ø± Ù…ÙƒÙˆÙ† Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù„Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„ÙŠÙ‡
        component_idx = random.randint(0, len(base_equation.components) - 1)
        target_component = base_equation.components[component_idx].copy()

        new_params = {'components': [comp.copy() for comp in base_equation.components]}

        if target_component['type'] == 'sigmoid':
            # ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù…Ù„ ÙˆØ§Ø­Ø¯
            param_to_improve = random.choice(['alpha', 'k', 'x0'])
            improvement_factor = 1 + random.uniform(-0.3, 0.3)

            new_params['components'][component_idx][param_to_improve] *= improvement_factor

            # ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ÙØªØ§Ø¦Ù„ Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØ§Ø²Ù†
            if param_to_improve == 'alpha':
                new_params['components'][component_idx]['alpha'] = max(0.1,
                    new_params['components'][component_idx]['alpha'])

        return new_params

    def _creative_exploration(self, base_equation: AdaptiveGSEEquation) -> Dict[str, Any]:
        """Ø§Ø³ØªÙƒØ´Ø§Ù Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ Ø¨ØªØ±ÙƒÙŠØ¨Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©"""

        new_params = {'components': []}

        # Ø¥Ø¶Ø§ÙØ© Ù…ÙƒÙˆÙ†Ø§Øª Ø¥Ø¨Ø¯Ø§Ø¹ÙŠØ©
        creativity_level = self.config.creativity_factor

        # Ù…ÙƒÙˆÙ† Ø³ÙŠØ¬Ù…ÙˆÙŠØ¯ Ø¥Ø¨Ø¯Ø§Ø¹ÙŠ
        creative_sigmoid = {
            'type': 'sigmoid',
            'alpha': random.uniform(0.5, 2.0) * (1 + creativity_level),
            'k': random.uniform(0.2, 3.0) * (1 + creativity_level),
            'x0': random.uniform(-3.0, 3.0) * (1 + creativity_level)
        }
        new_params['components'].append(creative_sigmoid)

        # Ù…ÙƒÙˆÙ† Ø®Ø·ÙŠ Ù…ÙƒÙ…Ù„
        complementary_linear = {
            'type': 'linear',
            'beta': random.uniform(-1.0, 1.0) * creativity_level,
            'gamma': random.uniform(-0.5, 0.5)
        }
        new_params['components'].append(complementary_linear)

        return new_params

    def _create_test_equation(self, params: Dict[str, Any]) -> AdaptiveGSEEquation:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ø®ØªØ¨Ø§Ø± Ù…Ù† Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª"""

        test_eq = AdaptiveGSEEquation()

        for component in params.get('components', []):
            if component['type'] == 'sigmoid':
                test_eq.add_sigmoid_component(
                    alpha=component['alpha'],
                    k=component['k'],
                    x0=component['x0']
                )
            elif component['type'] == 'linear':
                test_eq.add_linear_component(
                    beta=component['beta'],
                    gamma=component['gamma']
                )

        return test_eq

    def _analyze_discovered_pattern(self, params: Dict[str, Any],
                                  performance: float) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù…ÙƒØªØ´Ù"""

        pattern = {
            'parameters': params,
            'performance': performance,
            'discovery_time': datetime.now(),
            'pattern_signature': self._calculate_pattern_signature(params),
            'complexity': len(params.get('components', [])),
            'novelty_score': self._calculate_novelty_score(params)
        }

        return pattern

    def _calculate_pattern_signature(self, params: Dict[str, Any]) -> str:
        """Ø­Ø³Ø§Ø¨ Ø¨ØµÙ…Ø© Ø§Ù„Ù†Ù…Ø·"""

        signature_parts = []

        for component in params.get('components', []):
            if component['type'] == 'sigmoid':
                sig = f"S({component['alpha']:.2f},{component['k']:.2f},{component['x0']:.2f})"
            else:
                sig = f"L({component['beta']:.2f},{component['gamma']:.2f})"

            signature_parts.append(sig)

        return "+".join(signature_parts)

    def _calculate_novelty_score(self, params: Dict[str, Any]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¬Ø¯Ø© Ù„Ù„Ù†Ù…Ø·"""

        if not self.discovered_patterns:
            return 1.0

        # Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ÙƒØªØ´ÙØ© Ø³Ø§Ø¨Ù‚Ø§Ù‹
        current_signature = self._calculate_pattern_signature(params)

        similarity_scores = []
        for pattern in self.discovered_patterns:
            existing_signature = pattern.get('pattern_signature', '')
            similarity = self._calculate_signature_similarity(current_signature, existing_signature)
            similarity_scores.append(similarity)

        # Ø§Ù„Ø¬Ø¯Ø© = 1 - Ø£Ø¹Ù„Ù‰ ØªØ´Ø§Ø¨Ù‡
        max_similarity = max(similarity_scores) if similarity_scores else 0
        novelty = 1.0 - max_similarity

        return novelty

    def _calculate_signature_similarity(self, sig1: str, sig2: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ø¨ØµÙ…ØªÙŠÙ†"""

        # ØªØ´Ø§Ø¨Ù‡ Ø¨Ø³ÙŠØ· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ù…Ø´ØªØ±Ùƒ
        if not sig1 or not sig2:
            return 0.0

        common_length = 0
        min_length = min(len(sig1), len(sig2))

        for i in range(min_length):
            if sig1[i] == sig2[i]:
                common_length += 1
            else:
                break

        return common_length / max(len(sig1), len(sig2))

class IntegratedExpertExplorer:
    """
    Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„ Ù„Ù„Ø®Ø¨ÙŠØ± ÙˆØ§Ù„Ù…Ø³ØªÙƒØ´Ù

    ÙŠØ¯Ù…Ø¬ Ù‚Ø¯Ø±Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù ÙÙŠ Ù†Ø¸Ø§Ù… Ù…ÙˆØ­Ø¯
    """

    def __init__(self, exploration_config: ExplorationConfig = None):
        self.expert_system = GSEExpertSystem()
        self.explorer_system = GSEExplorerSystem(exploration_config)
        self.integration_history = []

        logger.info("ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„ Ù„Ù„Ø®Ø¨ÙŠØ± ÙˆØ§Ù„Ù…Ø³ØªÙƒØ´Ù")

    def intelligent_optimization(self, base_equation: AdaptiveGSEEquation,
                                x_data: np.ndarray, y_data: np.ndarray,
                                max_iterations: int = 10) -> Dict[str, Any]:
        """ØªØ­Ø³ÙŠÙ† Ø°ÙƒÙŠ Ù…ØªÙƒØ§Ù…Ù„"""

        logger.info("Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„")

        best_equation = base_equation
        best_performance = base_equation.calculate_error(x_data, y_data)

        optimization_history = []

        for iteration in range(max_iterations):
            logger.info(f"Ø§Ù„ØªÙƒØ±Ø§Ø± {iteration + 1}/{max_iterations}")

            # 1. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø¨ÙŠØ± Ù„Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ
            expert_analysis = self.expert_system.analyze_data_pattern(x_data, y_data)

            # 2. Ø§Ø³ØªÙƒØ´Ø§Ù Ù…ÙˆØ¬Ù‡ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø¨ÙŠØ±
            if expert_analysis.confidence > 0.7:
                exploration_mode = ExplorerMode.FOCUSED
            elif expert_analysis.confidence > 0.4:
                exploration_mode = ExplorerMode.GUIDED
            else:
                exploration_mode = ExplorerMode.CREATIVE

            exploration_result = self.explorer_system.explore_parameter_space(
                best_equation, x_data, y_data, exploration_mode
            )

            # 3. ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØ§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£ÙØ¶Ù„
            if exploration_result.performance_scores:
                best_exploration_idx = np.argmin(exploration_result.performance_scores)
                best_exploration_performance = exploration_result.performance_scores[best_exploration_idx]

                if best_exploration_performance < best_performance:
                    # ØªØ­Ø¯ÙŠØ« Ø£ÙØ¶Ù„ Ù…Ø¹Ø§Ø¯Ù„Ø©
                    best_params = exploration_result.new_parameters[best_exploration_idx]
                    best_equation = self.explorer_system._create_test_equation(best_params)
                    best_performance = best_exploration_performance

                    logger.info(f"ØªØ­Ø³Ù† ÙÙŠ Ø§Ù„ØªÙƒØ±Ø§Ø± {iteration + 1}: {best_performance:.6f}")

            # ØªØ³Ø¬ÙŠÙ„ ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ­Ø³ÙŠÙ†
            iteration_result = {
                'iteration': iteration + 1,
                'expert_analysis': expert_analysis,
                'exploration_result': exploration_result,
                'best_performance': best_performance,
                'improvement': base_equation.calculate_error(x_data, y_data) - best_performance
            }

            optimization_history.append(iteration_result)

        final_result = {
            'best_equation': best_equation,
            'best_performance': best_performance,
            'total_improvement': base_equation.calculate_error(x_data, y_data) - best_performance,
            'optimization_history': optimization_history,
            'expert_insights': self.expert_system.knowledge_base,
            'exploration_statistics': {
                'total_explorations': self.explorer_system.total_explorations,
                'successful_explorations': self.explorer_system.successful_explorations,
                'success_rate': self.explorer_system.successful_explorations / max(1, self.explorer_system.total_explorations)
            }
        }

        self.integration_history.append(final_result)

        logger.info(f"Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ: ØªØ­Ø³Ù† Ø¥Ø¬Ù…Ø§Ù„ÙŠ = {final_result['total_improvement']:.6f}")

        return final_result

if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø¨ÙŠØ±/Ø§Ù„Ù…Ø³ØªÙƒØ´Ù
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø¨ÙŠØ±/Ø§Ù„Ù…Ø³ØªÙƒØ´Ù")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø¨ÙŠØ±
    expert = GSEExpertSystem()
    
    # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø®ØªØ¨Ø§Ø±
    x_data = np.linspace(0, 10, 50)
    y_data = np.sin(x_data) + 0.1 * np.random.randn(50)
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø¨ÙŠØ±
    analysis = expert.analyze_data_pattern(x_data, y_data)
    print(f"ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø¨ÙŠØ±: Ù†Ù…Ø·={analysis.pattern_type}, Ø«Ù‚Ø©={analysis.confidence:.2%}")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø³ØªÙƒØ´Ù
    explorer = GSEExplorerSystem()
    
    # Ù…Ø¹Ø§Ø¯Ù„Ø© Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø§Ø³ØªÙƒØ´Ø§Ù
    base_eq = AdaptiveGSEEquation()
    base_eq.add_sigmoid_component(alpha=1.0, k=1.0, x0=0.0)
    
    # Ø§Ø³ØªÙƒØ´Ø§Ù
    exploration_result = explorer.explore_parameter_space(
        base_eq, x_data, y_data, ExplorerMode.GUIDED
    )
    
    print(f"Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù: Ø§ÙƒØªØ´Ø§ÙØ§Øª={len(exploration_result.discovered_patterns)}")
    print(f"Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {exploration_result.success_rate:.2%}")
    
    print("âœ… Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø¨ÙŠØ±/Ø§Ù„Ù…Ø³ØªÙƒØ´Ù Ù…ÙƒØªÙ…Ù„!")
