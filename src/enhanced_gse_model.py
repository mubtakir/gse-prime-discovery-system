#!/usr/bin/env python3
"""
Ù†Ù…ÙˆØ°Ø¬ GSE Ø§Ù„Ù…Ø­Ø³Ù† Ø¨Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø« ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØªÙƒÙŠÙÙŠ
ÙŠØ¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø³ØªÙˆØ­Ø§Ø© Ù…Ù† Ù†Ø¸Ø§Ù… Baserah

Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©:
- ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø« (Ø§Ù„ØªÙˆØ§Ø²Ù†ØŒ Ø§Ù„ØªØ¹Ø§Ù…Ø¯ØŒ Ø§Ù„ÙØªØ§Ø¦Ù„)
- Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ù…ØªÙƒÙŠÙØ© ØªØªØ·ÙˆØ± ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
- Ù†Ø¸Ø§Ù… Ø®Ø¨ÙŠØ±/Ù…Ø³ØªÙƒØ´Ù Ù„Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù
- ØªØ­Ø³ÙŠÙ† Ø°ÙƒÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª
- Ø­ÙØ¸ ÙˆØªØªØ¨Ø¹ ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ·ÙˆÙŠØ±
"""

import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Union
import logging
from datetime import datetime
import json
import pickle

try:
    from .gse_advanced_model import AdvancedGSEModel
    from .three_theories_core import ThreeTheoriesIntegrator
    from .adaptive_equations import AdaptiveGSEEquation, AdaptationDirection, AdaptationConfig
    from .expert_explorer_system import IntegratedExpertExplorer, ExplorationConfig
except ImportError:
    from gse_advanced_model import AdvancedGSEModel
    from three_theories_core import ThreeTheoriesIntegrator
    from adaptive_equations import AdaptiveGSEEquation, AdaptationDirection, AdaptationConfig
    from expert_explorer_system import IntegratedExpertExplorer, ExplorationConfig

logger = logging.getLogger(__name__)

class EnhancedGSEModel(AdvancedGSEModel):
    """
    Ù†Ù…ÙˆØ°Ø¬ GSE Ø§Ù„Ù…Ø­Ø³Ù† Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØªÙƒÙŠÙÙŠ ÙˆØ§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø«
    
    ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ†:
    - Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ GSE
    - Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø« Ø§Ù„Ù…Ø³ØªÙˆØ­Ø§Ø© Ù…Ù† Baserah
    - Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ù…ØªÙƒÙŠÙØ©
    - Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø¨ÙŠØ±/Ø§Ù„Ù…Ø³ØªÙƒØ´Ù
    """
    
    def __init__(self, adaptation_config: AdaptationConfig = None,
                 exploration_config: ExplorationConfig = None,
                 enable_theories: bool = True):
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        super().__init__()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†
        self.adaptation_config = adaptation_config or AdaptationConfig()
        self.exploration_config = exploration_config or ExplorationConfig()
        self.enable_theories = enable_theories
        
        # Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø«
        if self.enable_theories:
            self.theories_integrator = ThreeTheoriesIntegrator()
        
        # Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ù…ØªÙƒÙŠÙØ©
        self.adaptive_equations: List[AdaptiveGSEEquation] = []
        self.primary_adaptive_equation: Optional[AdaptiveGSEEquation] = None
        
        # Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø¨ÙŠØ±/Ø§Ù„Ù…Ø³ØªÙƒØ´Ù
        self.expert_explorer = IntegratedExpertExplorer(exploration_config)
        
        # ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ·ÙˆÙŠØ±
        self.enhancement_history = []
        self.performance_timeline = []
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†
        self.total_enhancements = 0
        self.successful_enhancements = 0
        self.adaptation_cycles = 0
        
        # Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        self.is_enhanced = False
        self.enhancement_level = 0
        self.best_performance = float('inf')
        
        logger.info("ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ GSE Ø§Ù„Ù…Ø­Ø³Ù† Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØªÙƒÙŠÙÙŠ")

    def add_sigmoid_component(self, alpha: float = 1.0, k: float = 1.0, x0: float = 0.0):
        """Ø¥Ø¶Ø§ÙØ© Ù…ÙƒÙˆÙ† Ø³ÙŠØ¬Ù…ÙˆÙŠØ¯ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬"""

        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ù„Ù…ØµÙÙˆÙØ§Øª
        if not hasattr(self, 'alpha_values') or self.alpha_values is None:
            self.alpha_values = np.array([])
            self.k_values = np.array([])
            self.x0_values = np.array([])

        self.alpha_values = np.append(self.alpha_values, alpha)
        self.k_values = np.append(self.k_values, k)
        self.x0_values = np.append(self.x0_values, x0)

        logger.debug(f"Ø£Ø¶ÙŠÙ Ù…ÙƒÙˆÙ† Ø³ÙŠØ¬Ù…ÙˆÙŠØ¯: Î±={alpha}, k={k}, x0={x0}")

    def add_linear_component(self, beta: float = 1.0, gamma: float = 0.0):
        """Ø¥Ø¶Ø§ÙØ© Ù…ÙƒÙˆÙ† Ø®Ø·ÙŠ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬"""

        # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙƒÙˆÙ† Ø®Ø·ÙŠ
        if not hasattr(self, 'linear_components'):
            self.linear_components = []

        linear_component = {'beta': beta, 'gamma': gamma}
        self.linear_components.append(linear_component)

        logger.debug(f"Ø£Ø¶ÙŠÙ Ù…ÙƒÙˆÙ† Ø®Ø·ÙŠ: Î²={beta}, Î³={gamma}")

    def calculate_loss(self, x_data: np.ndarray, y_data: np.ndarray) -> float:
        """Ø­Ø³Ø§Ø¨ Ø®Ø·Ø£ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""

        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
            y_pred = self.evaluate(x_data)
            mse = np.mean((y_data - y_pred) ** 2)
            return mse
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø·Ø£: {e}")
            return float('inf')

    def _estimate_parameter_gradient(self, x_data: np.ndarray, y_data: np.ndarray,
                                   epsilon: float = 1e-6) -> np.ndarray:
        """ØªÙ‚Ø¯ÙŠØ± Ø§Ù„ØªØ¯Ø±Ø¬ Ù„Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª"""

        gradient = []
        current_loss = self.calculate_loss(x_data, y_data)

        # ØªØ¯Ø±Ø¬ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø£Ù„ÙØ§
        for i in range(len(self.alpha_values)):
            original_value = self.alpha_values[i]

            self.alpha_values[i] = original_value + epsilon
            new_loss = self.calculate_loss(x_data, y_data)

            grad = (new_loss - current_loss) / epsilon
            gradient.append(grad)

            self.alpha_values[i] = original_value

        # ØªØ¯Ø±Ø¬ Ù…Ø¹Ø§Ù…Ù„Ø§Øª k
        for i in range(len(self.k_values)):
            original_value = self.k_values[i]

            self.k_values[i] = original_value + epsilon
            new_loss = self.calculate_loss(x_data, y_data)

            grad = (new_loss - current_loss) / epsilon
            gradient.append(grad)

            self.k_values[i] = original_value

        # ØªØ¯Ø±Ø¬ Ù…Ø¹Ø§Ù…Ù„Ø§Øª x0
        for i in range(len(self.x0_values)):
            original_value = self.x0_values[i]

            self.x0_values[i] = original_value + epsilon
            new_loss = self.calculate_loss(x_data, y_data)

            grad = (new_loss - current_loss) / epsilon
            gradient.append(grad)

            self.x0_values[i] = original_value

        return np.array(gradient)

    def create_adaptive_equation_from_current_model(self) -> AdaptiveGSEEquation:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø§Ø¯Ù„Ø© Ù…ØªÙƒÙŠÙØ© Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø­Ø§Ù„ÙŠ"""
        
        adaptive_eq = AdaptiveGSEEquation(adaptation_config=self.adaptation_config)
        
        # ØªØ­ÙˆÙŠÙ„ Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø­Ø§Ù„ÙŠ Ø¥Ù„Ù‰ Ù…ÙƒÙˆÙ†Ø§Øª Ù…ØªÙƒÙŠÙØ©
        for i in range(len(self.alpha_values)):
            adaptive_eq.add_sigmoid_component(
                alpha=self.alpha_values[i],
                k=self.k_values[i],
                x0=self.x0_values[i],
                adaptive=True
            )
        
        # Ø¥Ø¶Ø§ÙØ© Ù…ÙƒÙˆÙ†Ø§Øª Ø®Ø·ÙŠØ© Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª
        if hasattr(self, 'linear_components') and self.linear_components:
            for linear_comp in self.linear_components:
                adaptive_eq.add_linear_component(
                    beta=linear_comp.get('beta', 1.0),
                    gamma=linear_comp.get('gamma', 0.0),
                    adaptive=True
                )
        
        self.adaptive_equations.append(adaptive_eq)
        
        if self.primary_adaptive_equation is None:
            self.primary_adaptive_equation = adaptive_eq
        
        logger.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø§Ø¯Ù„Ø© Ù…ØªÙƒÙŠÙØ© Ø¨Ù€ {len(adaptive_eq.components)} Ù…ÙƒÙˆÙ†Ø§Øª")
        
        return adaptive_eq
    
    def enhance_with_three_theories(self, x_data: np.ndarray, y_data: np.ndarray,
                                  max_enhancement_cycles: int = 5) -> Dict[str, Any]:
        """ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø«"""
        
        if not self.enable_theories:
            logger.warning("Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø« ØºÙŠØ± Ù…ÙØ¹Ù„Ø©")
            return {'success': False, 'reason': 'theories_disabled'}
        
        logger.info("Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø¨Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø«")
        
        enhancement_results = []
        initial_performance = self.calculate_loss(x_data, y_data)
        best_performance = initial_performance
        
        for cycle in range(max_enhancement_cycles):
            logger.info(f"Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ­Ø³ÙŠÙ† {cycle + 1}/{max_enhancement_cycles}")
            
            cycle_start_performance = self.calculate_loss(x_data, y_data)
            
            # 1. ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ØªÙˆØ§Ø²Ù†
            balance_improvement = self._apply_balance_theory_enhancement(x_data, y_data)
            
            # 2. ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ØªØ¹Ø§Ù…Ø¯
            perpendicular_improvement = self._apply_perpendicular_theory_enhancement(x_data, y_data)
            
            # 3. ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ÙØªØ§Ø¦Ù„
            filament_improvement = self._apply_filament_theory_enhancement(x_data, y_data)
            
            cycle_end_performance = self.calculate_loss(x_data, y_data)
            cycle_improvement = cycle_start_performance - cycle_end_performance
            
            cycle_result = {
                'cycle': cycle + 1,
                'balance_improvement': balance_improvement,
                'perpendicular_improvement': perpendicular_improvement,
                'filament_improvement': filament_improvement,
                'total_cycle_improvement': cycle_improvement,
                'performance_after_cycle': cycle_end_performance
            }
            
            enhancement_results.append(cycle_result)
            
            if cycle_end_performance < best_performance:
                best_performance = cycle_end_performance
                self.best_performance = best_performance
            
            # ÙØ­Øµ Ø§Ù„ØªÙ‚Ø§Ø±Ø¨
            if abs(cycle_improvement) < 1e-8:
                logger.info(f"ØªÙ‚Ø§Ø±Ø¨ ÙÙŠ Ø§Ù„Ø¯ÙˆØ±Ø© {cycle + 1}")
                break
            
            self.adaptation_cycles += 1
        
        total_improvement = initial_performance - best_performance
        
        final_result = {
            'success': True,
            'total_improvement': total_improvement,
            'improvement_percentage': (total_improvement / initial_performance) * 100,
            'cycles_completed': len(enhancement_results),
            'best_performance': best_performance,
            'enhancement_results': enhancement_results,
            'theories_applied': ['balance', 'perpendicular', 'filament']
        }
        
        self.enhancement_history.append(final_result)
        self.is_enhanced = True
        self.enhancement_level += 1
        
        if total_improvement > 0:
            self.successful_enhancements += 1
        
        self.total_enhancements += 1
        
        logger.info(f"Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ­Ø³ÙŠÙ†: ØªØ­Ø³Ù† Ø¥Ø¬Ù…Ø§Ù„ÙŠ = {total_improvement:.6f} ({(total_improvement/initial_performance)*100:.2f}%)")
        
        return final_result
    
    def _apply_balance_theory_enhancement(self, x_data: np.ndarray, y_data: np.ndarray) -> float:
        """ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ØªÙˆØ§Ø²Ù† Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        
        initial_loss = self.calculate_loss(x_data, y_data)
        
        # ØªÙˆØ§Ø²Ù† Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø£Ù„ÙØ§
        balanced_alphas = self.theories_integrator.zero_duality.balance_coefficients(
            self.alpha_values, target_balance=0.5
        )
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙˆØ§Ø²Ù†
        old_alphas = self.alpha_values.copy()
        self.alpha_values = np.array(balanced_alphas)
        
        new_loss = self.calculate_loss(x_data, y_data)
        improvement = initial_loss - new_loss
        
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠØ­Ø¯Ø« ØªØ­Ø³Ù†ØŒ Ø£Ø±Ø¬Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
        if improvement <= 0:
            self.alpha_values = old_alphas
            improvement = 0
        
        logger.debug(f"ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙˆØ§Ø²Ù†: {improvement:.6f}")
        return improvement
    
    def _apply_perpendicular_theory_enhancement(self, x_data: np.ndarray, y_data: np.ndarray) -> float:
        """ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ØªØ¹Ø§Ù…Ø¯ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        
        initial_loss = self.calculate_loss(x_data, y_data)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¯Ø±Ø¬ Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠ
        gradient = self._estimate_parameter_gradient(x_data, y_data)
        
        if len(gradient) == 0:
            return 0.0
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø®Ø·ÙˆØ© ØªØ­Ø³ÙŠÙ† Ù…ØªØ¹Ø§Ù…Ø¯Ø©
        current_params = np.concatenate([self.alpha_values, self.k_values, self.x0_values])
        
        if len(gradient) == len(current_params):
            new_params = self.theories_integrator.perpendicular_opt.perpendicular_optimization_step(
                current_params, gradient, learning_rate=0.01
            )
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
            n_components = len(self.alpha_values)
            old_params = current_params.copy()
            
            self.alpha_values = np.maximum(0.1, new_params[:n_components])  # ØªØ¬Ù†Ø¨ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø³Ø§Ù„Ø¨Ø©
            self.k_values = np.maximum(0.1, new_params[n_components:2*n_components])
            self.x0_values = new_params[2*n_components:3*n_components]
            
            new_loss = self.calculate_loss(x_data, y_data)
            improvement = initial_loss - new_loss
            
            # Ø¥Ø°Ø§ Ù„Ù… ÙŠØ­Ø¯Ø« ØªØ­Ø³Ù†ØŒ Ø£Ø±Ø¬Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
            if improvement <= 0:
                self.alpha_values = old_params[:n_components]
                self.k_values = old_params[n_components:2*n_components]
                self.x0_values = old_params[2*n_components:3*n_components]
                improvement = 0
            
            logger.debug(f"ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¹Ø§Ù…Ø¯: {improvement:.6f}")
            return improvement
        
        return 0.0
    
    def _apply_filament_theory_enhancement(self, x_data: np.ndarray, y_data: np.ndarray) -> float:
        """ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ÙØªØ§Ø¦Ù„ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        
        initial_loss = self.calculate_loss(x_data, y_data)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙƒÙˆÙ†Ø§Øª Ù„Ù„ÙØªØ§Ø¦Ù„
        components = []
        for i in range(len(self.alpha_values)):
            component = {
                'alpha': self.alpha_values[i],
                'k': self.k_values[i],
                'x0': self.x0_values[i]
            }
            components.append(component)
        
        if len(components) < 2:
            return 0.0
        
        # ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙØªØ§Ø¦Ù„
        enhanced_alphas, enhanced_k, enhanced_x0 = self.theories_integrator.filament_connection.optimize_component_cooperation(
            self.alpha_values.tolist(), self.k_values.tolist(), self.x0_values.tolist()
        )
        
        # Ø­ÙØ¸ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
        old_alphas = self.alpha_values.copy()
        old_k = self.k_values.copy()
        old_x0 = self.x0_values.copy()
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­Ø³ÙŠÙ†
        self.alpha_values = np.array(enhanced_alphas)
        self.k_values = np.array(enhanced_k)
        self.x0_values = np.array(enhanced_x0)
        
        new_loss = self.calculate_loss(x_data, y_data)
        improvement = initial_loss - new_loss
        
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠØ­Ø¯Ø« ØªØ­Ø³Ù†ØŒ Ø£Ø±Ø¬Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
        if improvement <= 0:
            self.alpha_values = old_alphas
            self.k_values = old_k
            self.x0_values = old_x0
            improvement = 0
        
        logger.debug(f"ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙØªØ§Ø¦Ù„: {improvement:.6f}")
        return improvement
    
    def intelligent_adaptive_optimization(self, x_data: np.ndarray, y_data: np.ndarray,
                                        max_iterations: int = 10) -> Dict[str, Any]:
        """ØªØ­Ø³ÙŠÙ† Ø°ÙƒÙŠ ØªÙƒÙŠÙÙŠ Ø´Ø§Ù…Ù„"""
        
        logger.info("Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„ØªÙƒÙŠÙÙŠ Ø§Ù„Ø´Ø§Ù…Ù„")
        
        # 1. Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø§Ø¯Ù„Ø© Ù…ØªÙƒÙŠÙØ© Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø­Ø§Ù„ÙŠ
        adaptive_eq = self.create_adaptive_equation_from_current_model()
        
        # 2. ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ Ø¨Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø¨ÙŠØ±/Ø§Ù„Ù…Ø³ØªÙƒØ´Ù
        expert_result = self.expert_explorer.intelligent_optimization(
            adaptive_eq, x_data, y_data, max_iterations
        )
        
        # 3. ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø« Ù„Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¥Ø¶Ø§ÙÙŠ
        theories_result = self.enhance_with_three_theories(x_data, y_data)
        
        # 4. Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        if expert_result['best_equation']:
            self._update_model_from_adaptive_equation(expert_result['best_equation'])
        
        # 5. Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ­Ø³Ù† Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ
        final_performance = self.calculate_loss(x_data, y_data)
        
        comprehensive_result = {
            'success': True,
            'final_performance': final_performance,
            'expert_explorer_result': expert_result,
            'theories_enhancement_result': theories_result,
            'adaptive_equations_created': len(self.adaptive_equations),
            'enhancement_level': self.enhancement_level,
            'total_enhancements': self.total_enhancements,
            'successful_enhancements': self.successful_enhancements,
            'success_rate': self.successful_enhancements / max(1, self.total_enhancements)
        }
        
        self.performance_timeline.append({
            'timestamp': datetime.now(),
            'performance': final_performance,
            'enhancement_type': 'comprehensive_intelligent_adaptive'
        })
        
        logger.info(f"Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø´Ø§Ù…Ù„: Ø£Ø¯Ø§Ø¡ Ù†Ù‡Ø§Ø¦ÙŠ = {final_performance:.6f}")
        
        return comprehensive_result
    
    def _update_model_from_adaptive_equation(self, adaptive_eq: AdaptiveGSEEquation):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ù…Ø¹Ø§Ø¯Ù„Ø© Ù…ØªÙƒÙŠÙØ©"""
        
        sigmoid_components = [comp for comp in adaptive_eq.components if comp['type'] == 'sigmoid']
        
        if sigmoid_components:
            self.alpha_values = np.array([comp['alpha'] for comp in sigmoid_components])
            self.k_values = np.array([comp['k'] for comp in sigmoid_components])
            self.x0_values = np.array([comp['x0'] for comp in sigmoid_components])
            
            logger.info(f"ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ù…Ø¹Ø§Ø¯Ù„Ø© Ù…ØªÙƒÙŠÙØ©: {len(sigmoid_components)} Ù…ÙƒÙˆÙ†Ø§Øª")

if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù† GSE")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†
    enhanced_model = EnhancedGSEModel()
    
    # Ø¥Ø¶Ø§ÙØ© Ù…ÙƒÙˆÙ†Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©
    enhanced_model.add_sigmoid_component(alpha=1.0, k=1.0, x0=0.0)
    enhanced_model.add_sigmoid_component(alpha=0.8, k=0.5, x0=2.0)
    
    # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø®ØªØ¨Ø§Ø± (Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ø§Ù„Ø£ÙˆÙ„Ù‰)
    primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]
    x_data = np.array(range(1, len(primes) + 1))
    y_data = np.array([1 if i+1 in primes else 0 for i in range(len(primes))])
    
    print(f"Ø£Ø¯Ø§Ø¡ Ø£ÙˆÙ„ÙŠ: {enhanced_model.calculate_loss(x_data, y_data):.6f}")
    
    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø´Ø§Ù…Ù„
    result = enhanced_model.intelligent_adaptive_optimization(x_data, y_data, max_iterations=5)
    
    print(f"Ø£Ø¯Ø§Ø¡ Ù†Ù‡Ø§Ø¦ÙŠ: {result['final_performance']:.6f}")
    print(f"Ù…Ø¹Ø¯Ù„ Ù†Ø¬Ø§Ø­ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª: {result['success_rate']:.2%}")
    print(f"Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ­Ø³ÙŠÙ†: {result['enhancement_level']}")
    
    print("âœ… Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù† Ù…ÙƒØªÙ…Ù„!")
